{"cells":[{"cell_type":"markdown","metadata":{"id":"oDSof78w1aNs"},"source":["# SGAI models (DQN)"]},{"cell_type":"markdown","metadata":{"id":"dHLhWGLi1aNx"},"source":["This notebook is based off of the pytorch tutorial [here](https://pytorch.org/tutorials/intermediate/reinforcement_q_learning.html). It is intended to both create and train models for Courtney2-Outbreak"]},{"cell_type":"markdown","metadata":{"id":"HlKSg_ko1aNx"},"source":["### Setup"]},{"cell_type":"code","execution_count":1,"metadata":{"executionInfo":{"elapsed":1786,"status":"ok","timestamp":1658947050947,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"h4ClnufE3Ghv"},"outputs":[],"source":["# Load the TensorBoard notebook extension\n","%load_ext tensorboard"]},{"cell_type":"code","execution_count":2,"metadata":{"executionInfo":{"elapsed":2688,"status":"ok","timestamp":1658947056742,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"qYOE-D2i1aNy"},"outputs":[],"source":["import sys\n","import numpy as np\n","import tensorflow as tf\n","import keras.layers as layers\n","import keras.models as models\n","import keras\n","from collections import namedtuple, Counter\n","from queue import deque\n","import random\n","import math\n","from typing import List\n","from tqdm import tqdm  # used for progress meters\n","\n","PREFIX = \"SGAI_MK3\"\n","# make sure that it is able to import Board\n","sys.path.append(PREFIX)\n","\n","from Board import Board\n","from constants import *\n","from Player import ZombiePlayer, GovernmentPlayer\n"]},{"cell_type":"code","execution_count":3,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":13,"status":"ok","timestamp":1658947056743,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"Y4c8bDps1aNz","outputId":"5d89aa1b-c23d-4c80-a251-d74ab3a55470"},"outputs":[{"name":"stdout","output_type":"stream","text":["[PhysicalDevice(name='/physical_device:CPU:0', device_type='CPU')]\n"]}],"source":["DEVICE = \"CPU\"\n","# tf.debugging.set_log_device_placement(True)\n","devices = tf.config.list_physical_devices(DEVICE)\n","print(devices)\n","if DEVICE == \"GPU\":\n","    tf.config.experimental.set_memory_growth(devices[0], True)\n"]},{"cell_type":"markdown","metadata":{"id":"Z2HFphcH1aN0"},"source":["### Training Environments"]},{"cell_type":"code","execution_count":4,"metadata":{"executionInfo":{"elapsed":12,"status":"ok","timestamp":1658947056743,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"lMZ3ZIwW1aN1"},"outputs":[],"source":["class GovernmentEnvironment:\n","    ACTION_SPACE = tuple(range(8))\n","    ACTION_MAPPINGS = {\n","        0: \"moveUp\",\n","        1: \"moveDown\",\n","        2: \"moveLeft\",\n","        3: \"moveRight\",\n","        4: \"wallUp\",\n","        5: \"wallDown\",\n","        6: \"wallLeft\",\n","        7: \"wallRight\",\n","        8: \"vaccinate\",\n","        9: \"cureUp\",\n","        10: \"cureDown\",\n","        11: \"cureLeft\",\n","        12: \"cureRight\",\n","    }\n","    SIZE = (6, 6)\n","\n","    def __init__(self, max_timesteps: int = 300, logdir: str = \"\", run_name=\"\") -> None:\n","        self.max_timesteps = max_timesteps\n","        self.reset()\n","        self.total_timesteps = 0\n","        self.total_invalid_moves = 0\n","        self.writer = None\n","        if logdir != \"\" and run_name != \"\":\n","            self.writer = tf.summary.create_file_writer(f\"{logdir}/{run_name}\")\n","\n","    def reset(self):\n","        self.board = Board(GovernmentEnvironment.SIZE, \"Government\")\n","        self.board.populate(num_people=4, num_zombies=3)\n","        self.enemyPlayer = ZombiePlayer()\n","        self.done = False\n","\n","        # coordinates of the first Government player\n","        self.agentPosition = self.board.indexOf(False)\n","\n","        # useful for metrics\n","        self.max_number_of_government = 1\n","        self.episode_invalid_actions = 0\n","        self.episode_reward = 0\n","        self.episode_timesteps = 0\n","        return self._get_obs()\n","\n","    def step(self, action: int):\n","        action_name = GovernmentEnvironment.ACTION_MAPPINGS[action]\n","        # print(\"Before: \", end = str(self.agentPosition))\n","        # print()\n","        # print(action_name)\n","        if \"move\" in action_name:\n","            # print(self.board.get_board())\n","            valid, new_pos = self.board.actionToFunction[action_name](\n","                self.board.toCoord(self.agentPosition)\n","            )\n","            if valid:\n","                # print(self.board.get_board())\n","                # print(self.agentPosition)\n","                self.agentPosition = new_pos\n","                # print(\"After: \", end = str(self.agentPosition))\n","                # print()\n","        elif \"vaccinate\" in action_name:\n","            valid, _ = self.board.actionToFunction[action_name](\n","                self.board.toCoord(self.agentPosition)\n","            )\n","            if valid:\n","                self.board.vaccinate(self.agentPosition)\n","        elif \"cure\" in action_name:\n","            dest_coord = list(self.board.toCoord(self.agentPosition))\n","            if action_name == \"cureUp\":\n","                dest_coord[1] -= 1\n","            elif action_name == \"cureDown\":\n","                dest_coord[1] += 1\n","            elif action_name == \"cureRight\":\n","                dest_coord[0] += 1\n","            else:\n","                dest_coord[0] -= 1\n","            valid, _ = self.board.actionToFunction[\"cure\"](dest_coord)\n","        else:  # wall variation\n","            dest_coord = list(self.board.toCoord(self.agentPosition))\n","            if action_name == \"wallUp\":\n","                dest_coord[1] -= 1\n","            elif action_name == \"wallDown\":\n","                dest_coord[1] += 1\n","            elif action_name == \"wallRight\":\n","                dest_coord[0] += 1\n","            else:\n","                dest_coord[0] -= 1\n","            valid, _ = self.board.actionToFunction[\"wall\"](dest_coord)\n","    \n","        won = None\n","        # do the opposing player's action if the action was valid.\n","        if valid:\n","            _action, coord = self.enemyPlayer.get_move(self.board)\n","            if not _action:\n","                self.done = True\n","                won = True\n","            else:\n","                self.board.actionToFunction[_action](coord)\n","            self.board.update()\n","\n","        # see if the game is over\n","        # print(self.agentPosition)\n","        # print(self.board.get_board())\n","        # print(self._get_obs())\n","        if self.board.States[self.agentPosition].person is None:\n","            print(\"Lost Person\")\n","            \n","        if self.board.States[\n","            self.agentPosition\n","        ].person.isZombie:  # zombie was cured\n","            self.done = True\n","            won = False\n","        if not self.board.is_move_possible_at(self.agentPosition):  # no move possible\n","            self.done = True\n","        if self.episode_timesteps > self.max_timesteps:\n","            self.done = True\n","\n","        # get obs, reward, done, info\n","        obs, reward, done, info = (\n","            self._get_obs(),\n","            self._get_reward(action_name, valid, won),\n","            self._get_done(),\n","            self._get_info(),\n","        )\n","\n","        # update the metrics\n","        self.episode_reward += reward\n","        if not valid:\n","            self.episode_invalid_actions += 1\n","            self.total_invalid_moves += 1\n","        self.episode_timesteps += 1\n","        self.max_number_of_government = max(\n","            self.board.num_people(), self.max_number_of_government\n","        )\n","        self.total_timesteps += 1\n","\n","        # write the metrics\n","        if self.writer is not None:\n","            with self.writer.as_default():\n","                tf.summary.scalar(\n","                    \"train/invalid_action_rate\",\n","                    self.total_invalid_moves / self.total_timesteps,\n","                    step=self.total_timesteps,\n","                )\n","                tf.summary.scalar(\"train/cur_reward\", reward, step=self.total_timesteps)\n","\n","        # return the obs, reward, done, info\n","        return obs, reward, done, info\n","\n","    def _get_info(self):\n","        return {}\n","\n","    def _get_done(self):\n","        return self.done\n","\n","    def _get_reward(self, action_name: str, was_valid: bool, won: bool):\n","        \"\"\"\n","        Gonna try to return reward between [-1, 1]\n","        This fits w/i tanh and sigmoid ranges\n","        \"\"\"\n","        if not was_valid:\n","            return -1\n","        if won is True:\n","            return 1\n","        if won is False:\n","            return -0.5\n","        if \"wall\" in action_name:\n","            return 0.05\n","        if \"vaccinate\" in action_name:\n","            return 0.1\n","        if \"cure\" in action_name:\n","            return 0.2\n","        return 0.01  # this is the case where it was move\n","\n","    def _get_obs(self):\n","        \"\"\"\n","        Is based off the assumption that 5 is not in the returned board.\n","        Uses 5 as the key for current position.\n","        \"\"\"\n","        AGENT_POSITION_CONSTANT = 5\n","        ret = self.board.get_board()\n","        ret[self.agentPosition] = AGENT_POSITION_CONSTANT\n","        \n","        \"\"\"# normalize observation to be be centered at 0\n","        ret = np.array(ret, dtype=np.float32)\n","        ret /= np.float32(AGENT_POSITION_CONSTANT)\n","        ret -= np.float32(0.5)\"\"\"\n","        return np.array(ret)\n","\n","    def render(self):\n","        import PygameFunctions as PF\n","        import pygame\n","\n","        PF.run(self.board)\n","        pygame.display.update()\n","\n","    def init_render(self):\n","        import PygameFunctions as PF\n","        import pygame\n","\n","        PF.initScreen(self.board)\n","        pygame.display.update()\n","\n","    def close(self):\n","        import pygame\n","\n","        pygame.quit()\n","\n","    def write_run_metrics(self):\n","        if self.writer is not None:\n","            with self.writer.as_default():\n","                tf.summary.scalar(\n","                    \"episode/num_invalid_actions_per_ep\",\n","                    self.episode_invalid_actions,\n","                    step=self.total_timesteps,\n","                )\n","                tf.summary.scalar(\n","                    \"episode/episode_length\",\n","                    self.episode_timesteps,\n","                    step=self.total_timesteps,\n","                )\n","                tf.summary.scalar(\n","                    \"episode/episode_total_reward\",\n","                    self.episode_reward,\n","                    step=self.total_timesteps,\n","                )\n","                tf.summary.scalar(\n","                    \"episode/mean_reward\",\n","                    self.episode_reward / self.episode_timesteps,\n","                    step=self.total_timesteps,\n","                )\n","                tf.summary.scalar(\n","                    \"episode/percent_invalid_per_ep\",\n","                    self.episode_invalid_actions / self.episode_timesteps,\n","                    step=self.total_timesteps,\n","                )\n"]},{"cell_type":"markdown","metadata":{"id":"x08Uatnn1aN4"},"source":["### Make models"]},{"cell_type":"code","execution_count":5,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658947056743,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"3rDdDGwz1aN5"},"outputs":[],"source":["GOVERNMENT_OUTPUT_SIZE = len(GovernmentEnvironment.ACTION_SPACE)\n","INPUT_SHAPE = (ROWS * COLUMNS,)\n"]},{"cell_type":"code","execution_count":6,"metadata":{"executionInfo":{"elapsed":11,"status":"ok","timestamp":1658947056743,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"iQyURUmY1aN6"},"outputs":[],"source":["# from torch import conv2d\n","\n","\n","def make_government_model():\n","    \"\"\"\n","    makes the model that will be used for zombies\n","    The output of the model will be the predicted q value\n","    for being in a certain state.\n","    \"\"\"\n","    model = models.Sequential()\n","    model.add(layers.Reshape((6,6,1), input_shape = INPUT_SHAPE))\n","    model.add(layers.Conv2DTranspose(256, (3,3)))\n","    model.add(layers.Activation(\"relu\"))\n","    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n","    model.add(layers.Dropout(0.2))\n","\n","    model.add(layers.Conv2DTranspose(256, (3,3)))\n","    model.add(layers.Activation(\"relu\"))\n","    model.add(layers.MaxPooling2D(pool_size=(2,2)))\n","    model.add(layers.Dropout(0.2))\n","\n","    model.add(layers.Flatten())\n","    model.add(layers.Dense(64))\n","\n","    model.add(layers.Dense(GOVERNMENT_OUTPUT_SIZE, activation=\"linear\"))\n","\n","    return model\n","\n","\n","    # model = models.Sequential()\n","    # model.add(layers.InputLayer(INPUT_SHAPE))\n","    # model.add(layers.Flatten())\n","    # model.add(layers.Dense(36 * 2))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(36 * 4))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(36 * 8))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(36 * 16))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(36 * 32))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(GOVERNMENT_OUTPUT_SIZE * 16))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(GOVERNMENT_OUTPUT_SIZE * 8))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense( * 4))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(GOVERNMENT_OUTPUT_SIZE * 2))\n","    # model.add(layers.LeakyReLU())\n","    # model.add(layers.Dense(GOVERNMENT_OUTPUT_SIZE, activation='tanh'))\n","    # return model\n"]},{"cell_type":"code","execution_count":7,"metadata":{"executionInfo":{"elapsed":14,"status":"ok","timestamp":1658947057744,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"f1xLLzpq1aN7"},"outputs":[],"source":["with tf.device(DEVICE):\n","    gov_policy = make_government_model()\n","    gov_target = make_government_model()\n"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":14,"status":"ok","timestamp":1658947057745,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"yReU38Mq1aN8","outputId":"368a2f01-8801-4efb-cf7f-3ab26817c920"},"outputs":[{"name":"stdout","output_type":"stream","text":["(None, 36)\n","Model: \"sequential\"\n","_________________________________________________________________\n"," Layer (type)                Output Shape              Param #   \n","=================================================================\n"," reshape (Reshape)           (None, 6, 6, 1)           0         \n","                                                                 \n"," conv2d_transpose (Conv2DTra  (None, 8, 8, 256)        2560      \n"," nspose)                                                         \n","                                                                 \n"," activation (Activation)     (None, 8, 8, 256)         0         \n","                                                                 \n"," max_pooling2d (MaxPooling2D  (None, 4, 4, 256)        0         \n"," )                                                               \n","                                                                 \n"," dropout (Dropout)           (None, 4, 4, 256)         0         \n","                                                                 \n"," conv2d_transpose_1 (Conv2DT  (None, 6, 6, 256)        590080    \n"," ranspose)                                                       \n","                                                                 \n"," activation_1 (Activation)   (None, 6, 6, 256)         0         \n","                                                                 \n"," max_pooling2d_1 (MaxPooling  (None, 3, 3, 256)        0         \n"," 2D)                                                             \n","                                                                 \n"," dropout_1 (Dropout)         (None, 3, 3, 256)         0         \n","                                                                 \n"," flatten (Flatten)           (None, 2304)              0         \n","                                                                 \n"," dense (Dense)               (None, 64)                147520    \n","                                                                 \n"," dense_1 (Dense)             (None, 8)                 520       \n","                                                                 \n","=================================================================\n","Total params: 740,680\n","Trainable params: 740,680\n","Non-trainable params: 0\n","_________________________________________________________________\n"]}],"source":["print(gov_policy.input_shape)\n","gov_policy.summary()\n"]},{"cell_type":"code","execution_count":9,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":1008,"status":"ok","timestamp":1658947058749,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"ERSngyp11aN8","outputId":"64fba11e-1c9d-46ca-9b83-ce2356afdc42"},"outputs":[{"name":"stdout","output_type":"stream","text":["(1, 8)\n"]}],"source":["# make sure the output is correct shape\n","with tf.device(DEVICE):\n","    temp = gov_policy(tf.random.normal((1, 36)), training=False)\n","print(temp.shape)\n"]},{"cell_type":"markdown","metadata":{"id":"7gu0U4SL1aN9"},"source":["### Load saved model"]},{"cell_type":"code","execution_count":10,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658947058750,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"0gAyyEc-1aN9"},"outputs":[],"source":["# gov_policy.load_weights(\"gov_policy_weights\")\n","# gov_target.load_weights(\"gov_policy_weights\")\n"]},{"cell_type":"markdown","metadata":{"id":"v-COyfxd1aN-"},"source":["### DQN utilities"]},{"cell_type":"code","execution_count":11,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658947058750,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"kWt1kHve1aN-"},"outputs":[],"source":["# this acts as a class; useful in the training\n","Transition = namedtuple(\"Transition\", (\"state\", \"action\", \"next_state\", \"reward\"))\n","\n","\n","class ReplayMemory(object):\n","    def __init__(self, capacity):\n","        self.memory = deque([], maxlen=capacity)\n","\n","    def push(self, *args):\n","        \"\"\"Save a transition\"\"\"\n","        self.memory.append(Transition(*args))\n","\n","    def sample(self, batch_size):\n","        return random.sample(self.memory, batch_size)\n","\n","    def __len__(self):\n","        return len(self.memory)\n"]},{"cell_type":"markdown","metadata":{"id":"0RMr1Mou1aN-"},"source":["### Optimizers and Loss"]},{"cell_type":"code","execution_count":12,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658947058751,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"qju3Knxk1aN_"},"outputs":[],"source":["with tf.device(DEVICE):\n","    optimizer = tf.keras.optimizers.Adam(learning_rate=0.001)\n","    loss = keras.losses.MeanSquaredError()\n"]},{"cell_type":"markdown","metadata":{"id":"EPeSEHES1aN_"},"source":["### Training loop"]},{"cell_type":"code","execution_count":13,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658947058751,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"_ONoEWLu1aN_"},"outputs":[],"source":["BATCH_SIZE = 256\n","GAMMA = 0.999\n","EPSILON_MAX = 0.9  # exploration rate maximum\n","EPSILON_MIN = 0.05  # exploration rate minimum\n","EPS_DECAY = 1000  # decay rate, in steps\n","TARGET_UPDATE = 10  # how many episodes before the target is updated\n","\n","BUFFER_CAPACITY = 10000\n","memory = ReplayMemory(BUFFER_CAPACITY)\n"]},{"cell_type":"code","execution_count":14,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658947058751,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"pcMgWirG1aOA"},"outputs":[],"source":["def select_gov_action(state, steps_done: int = -1, writer=None):\n","    \"\"\"\n","    If no steps are provided, assuming not going to do\n","    random exploration\n","    \"\"\"\n","    sample = random.random()\n","    eps_threshold = 0\n","    if steps_done != -1:\n","        eps_threshold = EPSILON_MIN + (EPSILON_MAX - EPSILON_MIN) * math.exp(\n","            -1.0 * steps_done / EPS_DECAY\n","        )\n","    if writer is not None:\n","        with writer.as_default():\n","            tf.summary.scalar('exploration rate', eps_threshold, step=steps_done)\n","    if sample > eps_threshold:\n","        # Pick the action with the largest expected reward.\n","        temp = gov_policy(state, training=False)\n","        numpy = temp.numpy().flatten()\n","        return tf.constant([tuple(numpy).index(max(numpy))], dtype=tf.int32)\n","    else:\n","        return tf.constant([random.randrange(GOVERNMENT_OUTPUT_SIZE)], dtype=tf.int32)\n"]},{"cell_type":"code","execution_count":15,"metadata":{"executionInfo":{"elapsed":10,"status":"ok","timestamp":1658947058752,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"jfu1VqU51aOA"},"outputs":[],"source":["@tf.function\n","def train_on_batch(\n","    state_batch: tf.Tensor,\n","    action_batch: tf.Tensor,\n","    reward_batch: tf.Tensor,\n","    non_final_next_states: tf.Tensor,\n","    non_final_mask: tf.Tensor,\n","):\n","    with tf.GradientTape() as policy_tape:\n","        # Compute Q(s_t, a) - the model computes Q(s_t), then we select the\n","        # columns of actions taken. These are the actions which would've been taken\n","        # for each batch state according to policy_net\n","        action_batch = tf.expand_dims(action_batch, 1)\n","        state_action_values = tf.gather_nd(\n","            gov_policy(state_batch, training=True), action_batch, 1\n","        )\n","\n","        # Compute V(s_{t+1}) for all next states.\n","        # Expected values of actions for non_final_next_states are computed based\n","        # on the \"older\" target_net; selecting their best reward with max(1)[0].\n","        # This is merged based on the mask, such that we'll have either the expected\n","        # state value or 0 in case the state was final.\n","        next_state_values = tf.scatter_nd(\n","            tf.expand_dims(non_final_mask, 1),\n","            tf.reduce_max(gov_target(non_final_next_states, training=False), 1),\n","            tf.constant([BATCH_SIZE]),\n","        )\n","\n","        # Compute the expected Q values\n","        expected_state_action_values = tf.squeeze(\n","            (next_state_values * GAMMA) + reward_batch\n","        )\n","\n","        # compute loss (mean squared error)\n","        assert state_action_values.shape == expected_state_action_values.shape\n","        _loss = loss(state_action_values, expected_state_action_values)\n","\n","    # Optimize the model\n","    policy_gradient = policy_tape.gradient(_loss, gov_policy.trainable_variables)\n","\n","    # apply gradient\n","    optimizer.apply_gradients(zip(policy_gradient, gov_policy.trainable_variables))\n"]},{"cell_type":"code","execution_count":16,"metadata":{"executionInfo":{"elapsed":9,"status":"ok","timestamp":1658947058752,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"f8-z2UBk1aOB"},"outputs":[],"source":["def train(epochs, max_timesteps=200, render=False, logdir=\"\", run_name=\"\"):\n","    env = GovernmentEnvironment(max_timesteps, logdir, run_name)\n","    if render:\n","        env.init_render()\n","\n","    for episode in tqdm(range(epochs)):\n","        # Initialize the environment and state\n","        prev_obs = env.reset()\n","        done = False\n","        timesteps = 0\n","        while not done:\n","            if render:\n","                env.render()\n","\n","            # Select and perform an action\n","            action = select_gov_action(\n","                tf.constant([prev_obs]), env.total_timesteps, env.writer\n","            )\n","            action = action.numpy()[0]  # \"flatten\" the tensor and take the item\n","            new_obs, reward, done, _ = env.step(action)\n","            # reward = tf.constant([reward])\n","\n","            # Observe new state\n","            if not done:\n","                next_state = new_obs\n","            else:\n","                next_state = None\n","\n","            # Store the transition in memory\n","            memory.push(prev_obs, action, next_state, reward)\n","\n","            # Move to the next state\n","            prev_obs = next_state\n","\n","            # Perform one step of the optimization (on the policy network)\n","            if len(memory) >= BATCH_SIZE:\n","                # Transpose the batch (see https://stackoverflow.com/a/19343/3343043 for\n","                # detailed explanation). This converts batch-array of Transitions\n","                # to Transition of batch-arrays.\n","                batch = Transition(*zip(*memory.sample(BATCH_SIZE)))\n","\n","                # compute the states that aren't terminal states\n","                non_final_mask = tf.constant(\n","                    tuple(\n","                        idx\n","                        for state, idx in zip(\n","                            batch.next_state, range(len(batch.next_state))\n","                        )\n","                        if state is not None\n","                    ),\n","                )\n","                non_final_next_states = tf.cast(\n","                    tuple(state for state in batch.next_state if state is not None),\n","                    dtype=tf.float32,\n","                )\n","\n","                train_on_batch(\n","                    tf.cast(batch.state, dtype=tf.float32),\n","                    tf.cast(batch.action, dtype=tf.int32),\n","                    tf.cast(batch.reward, dtype=tf.float32),\n","                    non_final_next_states,\n","                    non_final_mask,\n","                )\n","\n","        env.write_run_metrics()\n","\n","        # Update the target network, copying all weights and biases in DQN\n","        if episode % TARGET_UPDATE == 0:\n","            gov_policy.save_weights(PREFIX+\"/gov_policy_weights\")\n","            gov_target.load_weights(PREFIX+\"/gov_policy_weights\")\n","    # env.close()\n","    gov_policy.save_weights(PREFIX+\"/gov_policy_weights\")\n"]},{"cell_type":"markdown","metadata":{"id":"39k9m2oD1aOB"},"source":["### Start Training!"]},{"cell_type":"code","execution_count":17,"metadata":{"executionInfo":{"elapsed":6,"status":"ok","timestamp":1658947065279,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"Ke1GsTS-1aOC"},"outputs":[],"source":["RUN_NUMBER = 1"]},{"cell_type":"code","execution_count":18,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"4Ly6UrS41aOC","outputId":"8135fb12-a09d-47a4-e4a3-8e2825a8fb5b"},"outputs":[{"name":"stderr","output_type":"stream","text":["  3%|▎         | 6/175 [00:00<00:07, 23.58it/s]2022-07-28 14:42:14.042182: W tensorflow/core/platform/profile_utils/cpu_utils.cc:128] Failed to get CPU frequency: 0 Hz\n"," 98%|█████████▊| 171/175 [19:23<00:34,  8.51s/it]"]}],"source":["for i in range(5):\n","    train(175, 100, render=False, logdir=\"GovernmentEnvironment\", run_name=f\"run{RUN_NUMBER}\")\n","    RUN_NUMBER+=1\n"]},{"cell_type":"markdown","metadata":{"id":"raXSWe531aOC"},"source":["### View Model Playing"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":2,"status":"ok","timestamp":1658947677163,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"JolyTwFm1aOD"},"outputs":[],"source":["def watch_model(max_timesteps=200):\n","    env = GovernmentEnvironment(max_timesteps)\n","    done = False\n","    #env.init_render()\n","    obs = env.reset()\n","    actions = []\n","    while not done:\n","        #env.render()\n","        action = select_gov_action(tf.constant([obs])).numpy()[0]\n","        obs, reward, done, _ = env.step(action)\n","        actions.append(action)\n","    #env.close()\n","    counter = Counter(actions)\n","    print(counter.most_common())\n"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"elapsed":836,"status":"ok","timestamp":1658947692074,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"owdHIiqX1aOD","outputId":"31505c9c-b744-411f-950f-8989cb3f6193"},"outputs":[{"name":"stdout","output_type":"stream","text":["[(5, 202)]\n"]}],"source":["watch_model()\n"]},{"cell_type":"code","execution_count":null,"metadata":{"executionInfo":{"elapsed":4,"status":"ok","timestamp":1658947655996,"user":{"displayName":"Eliot Hall","userId":"02758831649650665126"},"user_tz":420},"id":"Masw6LD61aOD"},"outputs":[],"source":[]}],"metadata":{"accelerator":"GPU","colab":{"name":"model.ipynb","provenance":[]},"gpuClass":"standard","kernelspec":{"display_name":"Python 3.10.5 64-bit","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.10.5"},"orig_nbformat":4,"vscode":{"interpreter":{"hash":"7e1998ff7f8aa20ada591c520b972326324e5ea05489af9e422744c7c09f6dad"}}},"nbformat":4,"nbformat_minor":0}
